Bert:
pre-training of deep bidirectional transformers for language understanding

BERT: Bidirectional Encoder Representations from Transformers
包括：
	预训练
	双向的编码特征
	深度transformer结构
	以语言模型为训练目标

文本表征：
	文本表征是文本的“数字形式“表达：
	
	自然语言是离散的
	
	文本表征：one-hot, tf-idf
	word2vec, doc2vec, glove, fastext...
	
词向量
	分布式假设：
		相同上下文语境的词有相似的含义
	词向量的弊端：
		每个词一个固定表征
	结合上下文的动态特征：
		ELMo GPT BERT等基于语言模型的表征
		语言模型：
			1.本质上概率模型
			2.隐马尔可夫链
			3.训练语言的套路
一些细节
 预训练数据的构建
  输入的两个部分都是包含完整句子
  同一句子中不会同时Mask同一个词
  非连续的句子都是取自不同文档
  有一定的概率让输入的有效文本长度小于输入全长（利用padding补齐），可以有一定的几率不让512个充满实际文本，而是有概率使其补齐。
  对于超过全长的文本，随机从第一部分部分和第二部分部分的头和尾削减
 模型
  Encoder LM任务的输出后及格过一个gelu非线性层再进行LM Loss的计算
  Classify任务的输出后经过一个tanh非线性层再进行的二分类
  
语言模型可以用来做什么
 ELMo
 BERT
 GPT

实验观察
 Elmo
  百度百科+中文维基，18亿tokens
  batch size：64，length：128，256
 
实验观察
 一些观察
  在QA任务上的效果明显
  在短文本分类上效果不及预期
  BERT在不按完整句子构建span时不按完整句子构建span
   预测是否连续的任务收敛很快
   短句很多的时候ppl收敛会很慢
